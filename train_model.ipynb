{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import dns\n",
    "import json\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "import haversine as hs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the query-doc JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory path\n",
    "path_to_json = '/Users/emadarmiti/Desktop/cap-s5/places_ranking/es_qureries/'\n",
    "\n",
    "# define the query-document json file \n",
    "query_doc = json.loads('{}')\n",
    "\n",
    "# get all json files that exist in the directory\n",
    "for file_path in os.listdir(path_to_json):\n",
    "    \n",
    "    if file_path.endswith('.json'):\n",
    "        \n",
    "        # open the json file \n",
    "        with open(os.path.join(path_to_json,file_path)) as json_file:\n",
    "            \n",
    "            # append the json file, to get all of them in on json variable\n",
    "            query_doc.update(json.load(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23079"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mongodb(mongoDB_url, database_name, collection_name):\n",
    "    \"\"\"connect to the mongoDB \n",
    "\n",
    "    Args:\n",
    "        mongoDB_url : mongoDB endpoint url\n",
    "        database_name : database name\n",
    "        collection_name : collection name\n",
    "\n",
    "    Returns:\n",
    "        the database collection object\n",
    "    \"\"\"\n",
    "\n",
    "    # create mongodb client\n",
    "    mongoDB_client = pymongo.MongoClient(mongoDB_url)\n",
    "    \n",
    "    # get the database\n",
    "    tweets_database = mongoDB_client[database_name]\n",
    "\n",
    "    # get the collection\n",
    "    tweets_collection = tweets_database[collection_name]\n",
    "    \n",
    "    # return the collection object\n",
    "    return tweets_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mongoDB endpoint\n",
    "mongodb_url = \"mongodb://rama:rama@cluster0-shard-00-00.xlj8q.mongodb.net:27017,cluster0-shard-00-01.xlj8q.mongodb.net:27017,cluster0-shard-00-02.xlj8q.mongodb.net:27017/tweets?ssl=true&replicaSet=atlas-yi054u-shard-0&authSource=admin&retryWrites=true&w=majority\"\n",
    "\n",
    "# connect to the database\n",
    "tweets_collection = connect_mongodb(mongodb_url, \"tweets\", \"tweets2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the tweets that near to the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets_near_place(coordinates):\n",
    "    \"\"\"find the tweets within 100m radius from the passed coordinates\n",
    "\n",
    "    Args:\n",
    "       coordinates : lon,lat for the central point\n",
    "\n",
    "    Returns:\n",
    "        extracted info from the tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the body of the query\n",
    "    myquery =  [\n",
    "        \n",
    "      { \"$geoNear\": {\n",
    "          \n",
    "             \"near\": { \"type\": \"Point\", \"coordinates\": coordinates },\n",
    "             \"distanceField\": \"place.coordinates\",\n",
    "             \"maxDistance\": 100}}\n",
    "        \n",
    "        \n",
    "    ,{  \"$group\": { \n",
    "        \n",
    "            \"_id\": None,\n",
    "            \"tweets_count\": { \"$sum\": 1 },\n",
    "            \"tweets_average_length\" : { \"$avg\" : {\"$strLenCP\" : \"$tweet\"}},\n",
    "            \"replies_count\": { \"$sum\": \"replies_count\"},\n",
    "            \"retweets_count\": { \"$sum\": \"retweets_count\"}, \n",
    "            \"likes_count\": { \"$sum\": \"likes_count\"},\n",
    "            \"hashtags\": { \"$sum\": {\"$size\" : \"$hashtags\"}},\n",
    "            \"mentions\": { \"$sum\": {\"$size\" : \"$mentions\"}}}}]\n",
    "\n",
    "    # send the query and return the results\n",
    "    return list(tweets_collection.aggregate(myquery))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(documents, query, user_location):\n",
    "    \n",
    "    # define the list result of the documents data\n",
    "    documents_data = []\n",
    "    \n",
    "    # go over each document and gather its info\n",
    "    for doc in documents:\n",
    "       \n",
    "        # slice the location of the document\n",
    "        coordinates = list(doc['_source']['location'].values())\n",
    "        \n",
    "       \n",
    "        \n",
    "        # get the tweets\n",
    "        tweets = find_tweets_near_place(coordinates)\n",
    "        \n",
    "        # neglect the doc that don't have tweets\n",
    "        if len(tweets):\n",
    "            \n",
    "            # get the document name\n",
    "            document = doc['_source']['name']\n",
    "            \n",
    "            \n",
    "            # find the jaccard similarity\n",
    "            jaccard_entire = jaccard_similarity(query.split(\" \"), document.split(\" \"))\n",
    "            \n",
    "            # slice the first 3 characters and find the jaccard similaity\n",
    "            sub_query = [word[:3] for word in query.split(\" \")]\n",
    "            sub_document = [word[:3] for word in document.split(\" \")]\n",
    "            \n",
    "            sub_jaccard = jaccard_similarity(sub_query, sub_document)\n",
    "            \n",
    "            # check if the query and the document have the same prefix\n",
    "            prefix_match = query[:3] == document[:3]\n",
    "            \n",
    "            # create two tuples of coordinates\n",
    "            tweet_loc = (coordinates[0], coordinates[1])\n",
    "            user_loc = (user_location[0], user_location[1])\n",
    "\n",
    "            # get the distance between the user and the document\n",
    "            distance = hs.haversine(tweet_loc,user_loc)\n",
    "            \n",
    "            # define the dict for the document\n",
    "            doc_data = {\"query\" : query,\n",
    "                        \"document\" : document,\n",
    "                        \"query_length\" : len(query),\n",
    "                        \"document_length\" : len(doc['_source']['name']),\n",
    "                        \"jaccard_entire\" : jaccard_entire,\n",
    "                        \"sub_jaccard\" : sub_jaccard,\n",
    "                        \"prefix_match\" : prefix_match,\n",
    "                        \"elasticsearch_score\" : doc['_score'],\n",
    "                        \"distance\" : distance}\n",
    "            \n",
    "            \n",
    "            # add the tweets info\n",
    "            doc_data.update(tweets[0])\n",
    "            \n",
    "            # append the doc dict to the list result\n",
    "            documents_data.append(doc_data)\n",
    "    \n",
    "    # return the documents data\n",
    "    return documents_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the result dict for all query-doc \n",
    "tweets_query_doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over each query-doc and use the previous function to get the tweets info from the MongoDB\n",
    "for query in list(query_doc.keys())[:10]:\n",
    "    \n",
    "    # build the dataset for one query\n",
    "    query_doc_data = build_dataset(query_doc[query], query, [40.7128, -74.0060])\n",
    "    \n",
    "    # neglect the queries that don't have documents with tweets\n",
    "    if len(query_doc_data):\n",
    "        tweets_query_doc.extend(query_doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(tweets_query_doc)\n",
    "data = data.drop('_id', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>query_length</th>\n",
       "      <th>document_length</th>\n",
       "      <th>jaccard_entire</th>\n",
       "      <th>sub_jaccard</th>\n",
       "      <th>prefix_match</th>\n",
       "      <th>elasticsearch_score</th>\n",
       "      <th>distance</th>\n",
       "      <th>tweets_count</th>\n",
       "      <th>tweets_average_length</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dix/Adler</td>\n",
       "      <td>Johnathan Adler</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>10.596428</td>\n",
       "      <td>8.283963</td>\n",
       "      <td>17</td>\n",
       "      <td>75.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dix/Adler</td>\n",
       "      <td>Jonathan Adler</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>10.596423</td>\n",
       "      <td>7.736840</td>\n",
       "      <td>30</td>\n",
       "      <td>76.233333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Downt</td>\n",
       "      <td>Up &amp; Down</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>7.047873</td>\n",
       "      <td>2.970166</td>\n",
       "      <td>151</td>\n",
       "      <td>73.973510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Downt</td>\n",
       "      <td>Dont know</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>7.047838</td>\n",
       "      <td>1.883768</td>\n",
       "      <td>66</td>\n",
       "      <td>74.681818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downt</td>\n",
       "      <td>Chau Down</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>7.047823</td>\n",
       "      <td>3.380790</td>\n",
       "      <td>43</td>\n",
       "      <td>75.023256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Downt</td>\n",
       "      <td>Double Down Saloon</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>6.029442</td>\n",
       "      <td>2.016149</td>\n",
       "      <td>51</td>\n",
       "      <td>75.019608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EastRi</td>\n",
       "      <td>Kasuri</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>4.749061</td>\n",
       "      <td>172.513834</td>\n",
       "      <td>3</td>\n",
       "      <td>84.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EastRi</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>4.747308</td>\n",
       "      <td>8.641929</td>\n",
       "      <td>6</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EastR</td>\n",
       "      <td>East Wok</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>4.434819</td>\n",
       "      <td>223.482080</td>\n",
       "      <td>6</td>\n",
       "      <td>66.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elk La</td>\n",
       "      <td>La La Taqueria</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>6.982998</td>\n",
       "      <td>32.076999</td>\n",
       "      <td>3</td>\n",
       "      <td>182.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Elk La</td>\n",
       "      <td>Oh La La</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>6.982845</td>\n",
       "      <td>24.839820</td>\n",
       "      <td>139</td>\n",
       "      <td>65.323741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Elk La</td>\n",
       "      <td>La La Land</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>6.982682</td>\n",
       "      <td>26.916188</td>\n",
       "      <td>5</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Empire Sta</td>\n",
       "      <td>Empire Auto Spa</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.497075</td>\n",
       "      <td>8.453954</td>\n",
       "      <td>14</td>\n",
       "      <td>63.928571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Empire Sta</td>\n",
       "      <td>Empire</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>9.207912</td>\n",
       "      <td>8.735253</td>\n",
       "      <td>4</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Empire Sta</td>\n",
       "      <td>Empire Visionworks</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>7.649227</td>\n",
       "      <td>292.500918</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Enla</td>\n",
       "      <td>Ella</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>9.932207</td>\n",
       "      <td>7.516779</td>\n",
       "      <td>31</td>\n",
       "      <td>76.258065</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Enla</td>\n",
       "      <td>Ella Baker School</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>7.053116</td>\n",
       "      <td>7.022646</td>\n",
       "      <td>50</td>\n",
       "      <td>67.980000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Esty</td>\n",
       "      <td>Zesty Pizzeria &amp; Salumeria</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>7.417501</td>\n",
       "      <td>9.156009</td>\n",
       "      <td>1262</td>\n",
       "      <td>65.385895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Esty</td>\n",
       "      <td>Kool-est Shoes</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>6.601116</td>\n",
       "      <td>21.824677</td>\n",
       "      <td>36</td>\n",
       "      <td>63.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         query                    document  query_length  document_length  \\\n",
       "0    Dix/Adler             Johnathan Adler             9               15   \n",
       "1    Dix/Adler              Jonathan Adler             9               14   \n",
       "2        Downt                   Up & Down             5                9   \n",
       "3        Downt                   Dont know             5                9   \n",
       "4        Downt                   Chau Down             5                9   \n",
       "5        Downt          Double Down Saloon             5               18   \n",
       "6       EastRi                      Kasuri             6                6   \n",
       "7       EastRi                     Eastern             6                7   \n",
       "8        EastR                    East Wok             5                8   \n",
       "9       Elk La              La La Taqueria             6               14   \n",
       "10      Elk La                    Oh La La             6                8   \n",
       "11      Elk La                  La La Land             6               10   \n",
       "12  Empire Sta             Empire Auto Spa            10               15   \n",
       "13  Empire Sta                      Empire            10                6   \n",
       "14  Empire Sta          Empire Visionworks            10               18   \n",
       "15        Enla                        Ella             4                4   \n",
       "16        Enla           Ella Baker School             4               17   \n",
       "17        Esty  Zesty Pizzeria & Salumeria             4               26   \n",
       "18        Esty              Kool-est Shoes             4               14   \n",
       "\n",
       "    jaccard_entire  sub_jaccard  prefix_match  elasticsearch_score  \\\n",
       "0         0.000000     0.000000         False            10.596428   \n",
       "1         0.000000     0.000000         False            10.596423   \n",
       "2         0.000000     0.333333         False             7.047873   \n",
       "3         0.000000     0.000000         False             7.047838   \n",
       "4         0.000000     0.500000         False             7.047823   \n",
       "5         0.000000     0.333333         False             6.029442   \n",
       "6         0.000000     0.000000         False             4.749061   \n",
       "7         0.000000     1.000000          True             4.747308   \n",
       "8         0.000000     0.500000          True             4.434819   \n",
       "9         0.333333     0.333333         False             6.982998   \n",
       "10        0.333333     0.333333         False             6.982845   \n",
       "11        0.333333     0.333333         False             6.982682   \n",
       "12        0.250000     0.250000          True             9.497075   \n",
       "13        0.500000     0.500000          True             9.207912   \n",
       "14        0.333333     0.333333          True             7.649227   \n",
       "15        0.000000     0.000000         False             9.932207   \n",
       "16        0.000000     0.000000         False             7.053116   \n",
       "17        0.000000     0.000000         False             7.417501   \n",
       "18        0.000000     0.000000         False             6.601116   \n",
       "\n",
       "      distance  tweets_count  tweets_average_length  replies_count  \\\n",
       "0     8.283963            17              75.117647              0   \n",
       "1     7.736840            30              76.233333              0   \n",
       "2     2.970166           151              73.973510              0   \n",
       "3     1.883768            66              74.681818              0   \n",
       "4     3.380790            43              75.023256              0   \n",
       "5     2.016149            51              75.019608              0   \n",
       "6   172.513834             3              84.333333              0   \n",
       "7     8.641929             6              68.000000              0   \n",
       "8   223.482080             6              66.833333              0   \n",
       "9    32.076999             3             182.333333              0   \n",
       "10   24.839820           139              65.323741              0   \n",
       "11   26.916188             5              63.000000              0   \n",
       "12    8.453954            14              63.928571              0   \n",
       "13    8.735253             4              74.750000              0   \n",
       "14  292.500918             1              61.000000              0   \n",
       "15    7.516779            31              76.258065              0   \n",
       "16    7.022646            50              67.980000              0   \n",
       "17    9.156009          1262              65.385895              0   \n",
       "18   21.824677            36              63.027778              0   \n",
       "\n",
       "    retweets_count  likes_count  hashtags  mentions  \n",
       "0                0            0         0        10  \n",
       "1                0            0         0        22  \n",
       "2                0            0         3        83  \n",
       "3                0            0         0        32  \n",
       "4                0            0         0        18  \n",
       "5                0            0         0        36  \n",
       "6                0            0         0         2  \n",
       "7                0            0         0         4  \n",
       "8                0            0         0         6  \n",
       "9                0            0         0         1  \n",
       "10               0            0         1        40  \n",
       "11               0            0         0         5  \n",
       "12               0            0         0         3  \n",
       "13               0            0         0         1  \n",
       "14               0            0         0         0  \n",
       "15               0            0         0         9  \n",
       "16               0            0         0        27  \n",
       "17               0            0         1       426  \n",
       "18               0            0         0         0  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'document', 'query_length', 'document_length',\n",
       "       'elasticsearch_score', 'distance', 'tweets_count',\n",
       "       'tweets_average_length', 'replies_count', 'retweets_count',\n",
       "       'likes_count', 'hashtags', 'mentions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('/Users/emadarmiti/Desktop/cap-s5/places_ranking/row_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### build the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the query-doc-tweets into a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf?\n",
    "# should we delete non tweets doc?\n",
    "# why just two featrues in labeling?\n",
    "# hashtags\n",
    "# query/document length\n",
    "# nlp\n",
    "# tweets average length\n",
    "# mentions - boolean\n",
    "## label = nlp + distance + elasicsearch + tweets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "victorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "term_frequency = victorizer.fit_transform(['emad','em'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_tranformer = TfidfTransformer(sublinear_tf=True, smooth_idf=False)\n",
    "\n",
    "   \n",
    "tf_idf = tf_idf_tranformer.fit_transform(term_frequency).T.todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
