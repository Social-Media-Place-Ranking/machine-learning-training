{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import dns\n",
    "import json\n",
    "from nltk.sentiment import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the query-doc JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory path\n",
    "path_to_json = '/Users/emadarmiti/Desktop/cap-s5/places_ranking/es_qureries/'\n",
    "\n",
    "# define the query-document json file \n",
    "query_doc = json.loads('{}')\n",
    "\n",
    "# get all json files that exist in the directory\n",
    "for file_path in os.listdir(path_to_json):\n",
    "    \n",
    "    if file_path.endswith('.json'):\n",
    "        \n",
    "        # open the json file \n",
    "        with open(os.path.join(path_to_json,file_path)) as json_file:\n",
    "            \n",
    "            # append the json file, to get all of them in on json variable\n",
    "            query_doc.update(json.load(json_file))\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23079"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mongodb(mongoDB_url, database_name, collection_name):\n",
    "    \"\"\"connect to the mongoDB \n",
    "\n",
    "    Args:\n",
    "        mongoDB_url : mongoDB endpoint url\n",
    "        database_name : database name\n",
    "        collection_name : collection name\n",
    "\n",
    "    Returns:\n",
    "        the database collection object\n",
    "    \"\"\"\n",
    "\n",
    "    # create mongodb client\n",
    "    mongoDB_client = pymongo.MongoClient(mongoDB_url)\n",
    "    \n",
    "    # get the database\n",
    "    tweets_database = mongoDB_client[database_name]\n",
    "\n",
    "    # get the collection\n",
    "    tweets_collection = tweets_database[collection_name]\n",
    "    \n",
    "    # return the collection object\n",
    "    return tweets_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mongoDB endpoint\n",
    "mongodb_url = \"mongodb://rama:rama@cluster0-shard-00-00.xlj8q.mongodb.net:27017,cluster0-shard-00-01.xlj8q.mongodb.net:27017,cluster0-shard-00-02.xlj8q.mongodb.net:27017/tweets?ssl=true&replicaSet=atlas-yi054u-shard-0&authSource=admin&retryWrites=true&w=majority\"\n",
    "\n",
    "# connect to the database\n",
    "tweets_collection = connect_mongodb(mongodb_url, \"tweets\", \"tweets2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the tweets that near to the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets_near_place(coordinates):\n",
    "    \"\"\"find the tweets within 100m radius from the passed coordinates\n",
    "\n",
    "    Args:\n",
    "       coordinates : lon,lat for the central point\n",
    "\n",
    "    Returns:\n",
    "        extracted info from the tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the body of the query\n",
    "    myquery =  [\n",
    "        \n",
    "      { \"$geoNear\": {\n",
    "          \n",
    "             \"near\": { \"type\": \"Point\", \"coordinates\": coordinates },\n",
    "             \"distanceField\": \"place.coordinates\",\n",
    "             \"maxDistance\": 100}}\n",
    "        \n",
    "        \n",
    "    ,{  \"$group\": { \n",
    "        \n",
    "            \"_id\": None,\n",
    "            \"tweets_count\": { \"$sum\": 1 },\n",
    "            \"replies_count\": { \"$sum\": \"replies_count\"},\n",
    "            \"retweets_count\": { \"$sum\": \"retweets_count\"}, \n",
    "            \"likes_count\": { \"$sum\": \"likes_count\"}}}]\n",
    "\n",
    "    # send the query and return the results\n",
    "    return list(tweets_collection.aggregate(myquery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(documents, query):\n",
    "    \n",
    "    # define the list result of the documents data\n",
    "    documents_data = []\n",
    "    \n",
    "    # go over each document and gather its info\n",
    "    for doc in documents:\n",
    "       \n",
    "        # slice the location of the document\n",
    "        coordinates = list(doc['_source']['location'].values())\n",
    "        \n",
    "        # get the tweets\n",
    "        tweets = find_tweets_near_place(coordinates)\n",
    "        \n",
    "        # neglect the doc that don't have tweets\n",
    "        if len(tweets):\n",
    "            \n",
    "            # define the dict for the document\n",
    "            doc_data = {\"query\" : query}\n",
    "            \n",
    "            # add the elasticsearch score\n",
    "            doc_data['elasticsearch_score'] = doc['_score']\n",
    "            \n",
    "            # add the document name and its location\n",
    "            doc_data.update({\"document\" : doc['_source']['name'], \n",
    "                               \"location\": doc['_source']['location']})\n",
    "            \n",
    "            # add the tweets info\n",
    "            doc_data.update(tweets[0])\n",
    "            \n",
    "            # append the doc dict to the list result\n",
    "            documents_data.append(doc_data)\n",
    "    \n",
    "    # return the documents data\n",
    "    return documents_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the result dict for all query-doc \n",
    "tweets_query_doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over each query-doc and use the previous function to get the tweets info from the MongoDB\n",
    "for query in list(query_doc.keys())[:3]:\n",
    "    \n",
    "    # build the dataset for one query\n",
    "    query_doc_data = build_dataset(query_doc[query], query)\n",
    "    \n",
    "    # neglect the queries that don't have documents with tweets\n",
    "    if len(query_doc_data):\n",
    "        tweets_query_doc.extend(query_doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>elasticsearch_score</th>\n",
       "      <th>document</th>\n",
       "      <th>_id</th>\n",
       "      <th>tweets_count</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>location.lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dix/Adler</td>\n",
       "      <td>10.596428</td>\n",
       "      <td>Johnathan Adler</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.778723</td>\n",
       "      <td>-73.960194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dix/Adler</td>\n",
       "      <td>10.596423</td>\n",
       "      <td>Jonathan Adler</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.779032</td>\n",
       "      <td>-73.977859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Downt</td>\n",
       "      <td>7.047873</td>\n",
       "      <td>Up &amp; Down</td>\n",
       "      <td>None</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.739328</td>\n",
       "      <td>-74.001875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Downt</td>\n",
       "      <td>7.047838</td>\n",
       "      <td>Dont know</td>\n",
       "      <td>None</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.704341</td>\n",
       "      <td>-73.986637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downt</td>\n",
       "      <td>7.047823</td>\n",
       "      <td>Chau Down</td>\n",
       "      <td>None</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.689735</td>\n",
       "      <td>-73.979871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Downt</td>\n",
       "      <td>6.029442</td>\n",
       "      <td>Double Down Saloon</td>\n",
       "      <td>None</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.722569</td>\n",
       "      <td>-73.985846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EastRi</td>\n",
       "      <td>4.749061</td>\n",
       "      <td>Kasuri</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.256250</td>\n",
       "      <td>-73.795907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EastRi</td>\n",
       "      <td>4.747308</td>\n",
       "      <td>Eastern</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.765673</td>\n",
       "      <td>-73.930822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       query  elasticsearch_score            document   _id  tweets_count  \\\n",
       "0  Dix/Adler            10.596428     Johnathan Adler  None            17   \n",
       "1  Dix/Adler            10.596423      Jonathan Adler  None            30   \n",
       "2      Downt             7.047873           Up & Down  None           151   \n",
       "3      Downt             7.047838           Dont know  None            66   \n",
       "4      Downt             7.047823           Chau Down  None            43   \n",
       "5      Downt             6.029442  Double Down Saloon  None            51   \n",
       "6     EastRi             4.749061              Kasuri  None             3   \n",
       "7     EastRi             4.747308             Eastern  None             6   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  location.lat  location.lon  \n",
       "0              0               0            0     40.778723    -73.960194  \n",
       "1              0               0            0     40.779032    -73.977859  \n",
       "2              0               0            0     40.739328    -74.001875  \n",
       "3              0               0            0     40.704341    -73.986637  \n",
       "4              0               0            0     40.689735    -73.979871  \n",
       "5              0               0            0     40.722569    -73.985846  \n",
       "6              0               0            0     42.256250    -73.795907  \n",
       "7              0               0            0     40.765673    -73.930822  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(tweets_query_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/emadarmiti/Desktop/cap-s5/places_ranking/row_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### build the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the query-doc-tweets into a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf?\n",
    "# should we delete non tweets doc?\n",
    "# why just two featrues in labeling?\n",
    "# hashtags\n",
    "# query/document length\n",
    "# nlp\n",
    "# tweets average length\n",
    "# mentions - boolean\n",
    "## label = nlp + distance + elasicsearch + tweets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
